# -*- coding: utf-8 -*-
"""finaluntilnow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kcVFD9wgzmNwRrogk_jPs2RgC3RosIOJ

#🔍 Deep Learning Competition – Competition 2

## 👩‍💻 Students: Tair Tobol & Maksi Krutinsky

### Step 1: Import Libraries and Setup
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score, classification_report, confusion_matrix
from tensorflow.keras import layers, models
from tensorflow.keras.layers import Input
import tensorflow.keras as keras

nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

"""### Step 2: Load Data and Text Preprocessing"""

train = pd.read_excel("task2_train.xlsx")
validation = pd.read_excel("task2_val.xlsx")
test = pd.read_excel("task2_test.xlsx")

print("Train shape:", train.shape)
print("Validation shape:", validation.shape)
print("Test shape:", test.shape)


def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"[^a-z\s]", "", text)
    tokens = word_tokenize(text)
    tokens = [t for t in tokens if t not in stop_words]
    return " ".join(tokens)

train['processed_text'] = train['text'].apply(clean_text)
validation['processed_text'] = validation['text'].apply(clean_text)
test['processed_text'] = test['text'].apply(clean_text)

print(train['processed_text'].head())

"""### Step 3: Text Vectorization using Tfidf



"""

import gensim.downloader as api, numpy as np
from sklearn.preprocessing import StandardScaler

# ---------- 1. load fast-text ----------
ft = api.load('fasttext-wiki-news-subwords-300')   # 300‑D

tok = lambda s: s.lower().split()
alpha = 1e-3

# הכנה ל‑SIF (IDF מ‑train בלבד)
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(tokenizer=tok, lowercase=False, min_df=2)
tfidf.fit(train['text'])
idf   = tfidf.idf_;  vocab = tfidf.vocabulary_

def doc_vec(sent):
    toks = [w for w in tok(sent) if w in ft]
    if not toks:
        return np.zeros(600)
    # משקל סופי לכל מילה – α/(α + idf)
    vecs = [ft[w] * (alpha / (alpha + idf[vocab.get(w, 0)])) for w in toks]
    v = np.vstack(vecs)
    vec = np.concatenate([v.mean(axis=0), v.min(axis=0)])  # mean+min  → 600‑D
    return vec / np.linalg.norm(vec)  # L2  (אין צורך בבדיקה, norm>0)

X_train = np.vstack(train['text'].apply(doc_vec))
X_val   = np.vstack(validation['text'].apply(doc_vec))
X_test  = np.vstack(test['text'].apply(doc_vec))

from sklearn.decomposition import TruncatedSVD
svd = TruncatedSVD(n_components=400, random_state=240)
X_train_p = svd.fit_transform(X_train)
X_val_p   = svd.transform(X_val)
X_test_p  = svd.transform(X_test)

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train_p)
X_val_scaled   = scaler.transform(X_val_p)
X_test_scaled  = scaler.transform(X_test_p)

"""### Step 4: Normalization"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train_clean = X_train[train['label'] == 0]
X_train_scaled = scaler.fit_transform(X_train_clean)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

print("\nNormalized train set shape:", X_train_scaled.shape)

"""### Step 5: Build and Train Autoencoder"""

from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Dropout, BatchNormalization


early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
input_dim = X_train_scaled.shape[1]

input_layer = Input(shape=(input_dim,))

# ----- Encoder -----
encoded = layers.Dense(128, activation='relu')(input_layer)
encoded = BatchNormalization()(encoded)
encoded = Dropout(0.3)(encoded)
encoded = layers.Dense(64, activation='relu')(encoded)
encoded = Dropout(0.2)(encoded)
encoded = layers.Dense(32, activation='relu')(encoded)
encoded = layers.Dense(16, activation='relu')(encoded)

# ----- Decoder -----
decoded = layers.Dense(32, activation='relu')(encoded)
decoded = layers.Dense(64, activation='relu')(decoded)
decoded = layers.Dense(input_dim, activation='sigmoid')(decoded)  # 120

autoencoder = models.Model(inputs=input_layer, outputs=decoded)
autoencoder.compile(optimizer='adam', loss='mse')



print("\n📌 training on X_train_scaled:")

# Train
history = autoencoder.fit(
    X_train_scaled, X_train_scaled,
    epochs=50,
    batch_size=128,
    shuffle=True,
    validation_data=(X_val_scaled, X_val_scaled),
    callbacks=[early_stop]
)



plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title("Loss over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid()

plt.subplot(1,2,2)
plt.plot(history.history.get('accuracy', []), label='Train Accuracy')
plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
plt.title("Accuracy over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid()
plt.tight_layout()
plt.show()

"""### Step 6: Error Analysis and Threshold Selection"""

val_reconstructed = autoencoder.predict(X_val_scaled)
mse_val = np.mean(np.power(X_val_scaled - val_reconstructed, 2), axis=1)
y_val_true = (validation['label'] > 0).astype(int)

plt.figure(figsize=(7,4))
plt.hist(mse_val, bins=50, color='purple', alpha=0.7)
plt.title("Reconstruction Error Histogram - Validation")
plt.xlabel("MSE")
plt.ylabel("Frequency")
plt.grid()
plt.show()

mse_normal = mse_val[validation['label'] == 0]

from sklearn.metrics import precision_recall_curve

precisions, recalls, thresholds = precision_recall_curve(y_val_true, mse_val)
f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)
best_idx = np.argmax(f1_scores)
best_threshold = thresholds[best_idx]
print(f"🔧 Best F1: {f1_scores[best_idx]:.4f} at threshold: {best_threshold:.4f}")

y_val_pred = (mse_val > best_threshold).astype(int)

plt.figure(figsize=(8,5))
plt.plot(thresholds, precisions[:-1], label='Precision')
plt.plot(thresholds, recalls[:-1], label='Recall')
plt.plot(thresholds, f1_scores[:-1], label='F1')
plt.xlabel("Threshold")
plt.ylabel("Score")
plt.title("Precision / Recall / F1 vs Threshold")
plt.grid(True)
plt.legend()
plt.show()

"""### Step 7: Classification Report and Evaluation"""

print("\nClassification Report:")
print(classification_report(y_val_true, y_val_pred, digits=4))

plt.figure(figsize=(6,5))
sns.heatmap(confusion_matrix(y_val_true, y_val_pred), annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix - Validation")
plt.show()

from sklearn.metrics import precision_score, recall_score

print(f"Final Precision (class 1): {precision_score(y_val_true, y_val_pred, pos_label=1):.4f}")
print(f"Final Recall (class 1): {recall_score(y_val_true, y_val_pred, pos_label=1):.4f}")
print(f"Final F1 (class 1): {f1_score(y_val_true, y_val_pred, pos_label=1):.4f}")

"""### Step 8: Visualization with PCA"""

pca_vis = PCA(n_components=2)
X_val_vis = pca_vis.fit_transform(X_val_scaled)

plt.figure(figsize=(8,6))
sns.scatterplot(x=X_val_vis[:,0], y=X_val_vis[:,1], hue=y_val_pred, palette='coolwarm', alpha=0.7)
plt.title("PCA Visualization of Validation Predictions")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.grid(True)
plt.show()

"""### Step 9: Predict on Test Set and Submit File"""

test_reconstructed = autoencoder.predict(X_test_scaled)
mse_test = np.mean(np.power(X_test_scaled - test_reconstructed, 2), axis=1)
y_test_pred = (mse_test > best_threshold).astype(int)

plt.figure(figsize=(7,4))
plt.hist(mse_test, bins=50, color='green', alpha=0.7)
plt.title("Reconstruction Error Histogram - Test")
plt.xlabel("MSE")
plt.ylabel("Frequency")
plt.grid()
plt.show()

submission = pd.DataFrame({
    'ID': test['ID'],
    'label': y_test_pred
})

from google.colab import files
files.download("task2_submission.csv")